spark-dataIngestion-example {

  kafka {
    topic = "torinoKafka"
    bootstrapServers = "127.0.0.1:9092"
    zookeeperServers = "127.0.0.1:2181"
    serializer = "org.apache.kafka.common.serialization.ByteArraySerializer"
    deserializer = "org.apache.kafka.common.serialization.ByteArrayDeserializer"
  }

  hdfs {
    filename_ = "hdfs://cdh1/user/kafka/iotingestion/torinoFDT"
    filename = "./torinoFDT"
  }

  openTSDB {
    saltWidth = 1
    saltBuckets = 4
    hbaseTable = "tsdb"
    hbaseUidTable = "tsdb-uid"
    autoCreateMetrics = "true"
    metric = 1
  }
  influx {
    uri = "http://localhost:8086"
    user = "myuser"
    password = "mypass"
    db_name = "timeseries"

  }

  hbase {
    master = "localhost:60000"
  }


}
